# Optimised-W8A16-Llama
Optimised Llama model with W8A16 quantised linear layers for faster inference and reduced memory usage
